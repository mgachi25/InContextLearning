# In-Context Learning
This project is an investigation into the ability for Transformer architectures to perform in context learning. While LLMs appear to be able to in-context learn, we can evaluate this characteristic of the architecture directly by training Transformers to perform in-context learning tasks.

In this project, we replicate the results from ["What Can Transformers Learn In-Context? A Case Study of Simple Function Classes"](https://arxiv.org/pdf/2208.01066) (Garg et. al., 2024). The results corroborate the paper's conclusions; namely, that Transformers can perform in-context learning of simple function classes with performance comparable to the optimal predictor.

This repo contains the .ipnyb file, and a link to the Colab notebook used can be found [here](https://colab.research.google.com/drive/1RQHX6JaZbNJiBNzD2O9te6NdnIq39XK8).


